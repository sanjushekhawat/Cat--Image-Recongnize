import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage
from lr_utils import load_dataset

%matplotlib inline

#Loading the data (cat/non-cat)
train_set_x_orig,Y, test_set_x_orig, Y_test, classes = load_dataset()

#Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)

train_set_x_flatten =train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

#One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).

X= train_set_x_flatten/255
X_test= test_set_x_flatten/255

def sigmoid(z):
    s =1/(1+np.exp(-z))
    return s

def initialize_with_zeros(dim):
    w = np.zeros((dim, 1))
    b=0
    return w, b

def propagate(w, b, X, Y):
    m = X.shape[1]
    A = sigmoid(np.dot(w.T,X)+b)                                     # compute activation
    cost = (-1 / m) * np.sum( Y * np.log(A) + (1-Y) * np.log(1-A) )                             # compute cost
    dw = (1 / m) * np.dot(X, (A - Y).T)       # Dimention = (num_px * num_px * 3, 1)
    db = (1 / m) * np.sum(A - Y)              # Dimention = Scalar 
    grads = {"dw": dw,
             "db": db}
    
    return grads, cost
    
num_iterations=50
learning_rate=0.001

    costs = []
    
    for i in range(num_iterations):
        grads, cost = propagate(w, b, X, Y)
        dw = grads["dw"]
        db = grads["db"}
        w = w - (learning_rate * dw)
        b = b - (learning_rate * db)
   
        if i % 100 == 0:
            costs.append(cost)
        
        # Print the cost every 100 training iterations
        if print_cost and i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))
    
    params = {"w": w,
              "b": b}
    
    grads = {"dw": dw,
             "db": db}
    
    def predict(x):
    xp=np.dot(x,w)+b
    test=np.full(xp.shape,0.5)
    test= xp>test
    return test*1
    
    
    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)

   
    # Print train/test Errors
    print("train accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
    print("test accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))

